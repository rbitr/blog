<h1 id="tensorflow-tutorial">TensorFlow Tutorial</h1>
<p>There are a lot of TensorFlow tutorials. I found the easy-to-find ones lacking in detail about what is going on behind the abstractions used in TF. My goal here is to present an exercise, analogous to this <a href="https://fluxml.ai/Flux.jl/stable/models/basics/"><code>flux.jl</code> introduction</a> that builds up from close to scratch to illustrate the different abstractions.</p>
<p>I used TF 2.3.1 for this. It assumes familiarity with Python and with gradient based neural network training, but not with TF.</p>
<h2 id="data">Data</h2>
<p>First, we will make up some simple fake data and plot it:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="im">import</span> tensorflow <span class="im">as</span> tf</a>
<a class="sourceLine" id="cb1-2" title="2"></a>
<a class="sourceLine" id="cb1-3" title="3"><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</a>
<a class="sourceLine" id="cb1-4" title="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb1-5" title="5"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-6" title="6"></a>
<a class="sourceLine" id="cb1-7" title="7"><span class="co"># made up data</span></a>
<a class="sourceLine" id="cb1-8" title="8">centers <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">-1</span>]]</a>
<a class="sourceLine" id="cb1-9" title="9">X, y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">1000</span>, centers<span class="op">=</span>centers, cluster_std<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-10" title="10"></a>
<a class="sourceLine" id="cb1-11" title="11"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb1-12" title="12"><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">set</span>(y):</a>
<a class="sourceLine" id="cb1-13" title="13">    x <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x,l <span class="kw">in</span> <span class="bu">zip</span>(X,y) <span class="cf">if</span> l<span class="op">==</span>k]</a>
<a class="sourceLine" id="cb1-14" title="14">    label <span class="op">=</span> [x[<span class="dv">1</span>] <span class="cf">for</span> x,l <span class="kw">in</span> <span class="bu">zip</span>(X,y) <span class="cf">if</span> l<span class="op">==</span>k]</a>
<a class="sourceLine" id="cb1-15" title="15">    plt.scatter(x,label)</a>
<a class="sourceLine" id="cb1-16" title="16">    </a>
<a class="sourceLine" id="cb1-17" title="17">plt.show()</a></code></pre></div>
<p><img src="data.png" title="Two classes" /></p>
<p>Our input, X is a list of 2D vectors, and our output is a list of 1’s or 0’s representing two different class labels corresponding to the orange and blue points in the plot.</p>
<h2 id="training-with-tfkeras">Training with TF/Keras</h2>
<p>Now, here is an example training a linear model on this data that roughly mirrors a lot of TF tutorials:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1">model <span class="op">=</span> tf.keras.models.Sequential([tf.keras.layers.Dense(<span class="dv">1</span>,input_shape<span class="op">=</span>(<span class="dv">2</span>,))])</a>
<a class="sourceLine" id="cb2-2" title="2">model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;SGD&#39;</span>,</a>
<a class="sourceLine" id="cb2-3" title="3">    loss<span class="op">=</span>tf.keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</a>
<a class="sourceLine" id="cb2-4" title="4">    metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</a>
<a class="sourceLine" id="cb2-5" title="5">model.fit(X,y,batch_size<span class="op">=</span><span class="dv">100</span>,epochs<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb2-6" title="6"></a>
<a class="sourceLine" id="cb2-7" title="7">Out[]: </a>
<a class="sourceLine" id="cb2-8" title="8"></a>
<a class="sourceLine" id="cb2-9" title="9">Epoch <span class="dv">1</span><span class="op">/</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb2-10" title="10"><span class="dv">10</span><span class="op">/</span><span class="dv">10</span> [<span class="op">==============================</span>] <span class="op">-</span> 0s 955us<span class="op">/</span>step <span class="op">-</span> loss: <span class="fl">0.9161</span> <span class="op">-</span> accuracy: <span class="fl">0.3250</span></a>
<a class="sourceLine" id="cb2-11" title="11">Epoch <span class="dv">2</span><span class="op">/</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb2-12" title="12"><span class="dv">10</span><span class="op">/</span><span class="dv">10</span> [<span class="op">==============================</span>] <span class="op">-</span> 0s 2ms<span class="op">/</span>step <span class="op">-</span> loss: <span class="fl">0.8414</span> <span class="op">-</span> accuracy: <span class="fl">0.3880</span></a>
<a class="sourceLine" id="cb2-13" title="13">...</a>
<a class="sourceLine" id="cb2-14" title="14">Epoch <span class="dv">20</span><span class="op">/</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb2-15" title="15"><span class="dv">10</span><span class="op">/</span><span class="dv">10</span> [<span class="op">==============================</span>] <span class="op">-</span> 0s 1ms<span class="op">/</span>step <span class="op">-</span> loss: <span class="fl">0.3626</span> <span class="op">-</span> accuracy: <span class="fl">0.8580</span></a></code></pre></div>
<p>TF/Keras does a lot for us here, but I never found it very satisfying following this kind of example. Reducing it to API calls means that understanding what is going on becomes about reading the API - this is not necessarily bad but for beginners it takes away any intuition about what is happening and why, and what to do if you wanted to change anything[^1].</p>
<p>[^1]For example, when I was writing this, I started by copying the <code>model.compile(loss='sparse_categorical_cross_entropy'...)</code> line from another tutorial. I wanted binary cross entropy, and I wanted it from logits, so I had to read the API to learn that specifying the loss with ‘binary_cross_entropy’ as a string assumes the output is a probability, but that I can instead specify the loss as an object as I did in the example above. Not a big deal, but not super intuitive either, especially if you are not sure what you are looking for.</p>
<h2 id="the-longer-way">The longer way</h2>
<p>So, let’s try to use TF as little as possible.</p>
<p>We cast the training data to TF constants as follows:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">X <span class="op">=</span> tf.constant(X,dtype<span class="op">=</span>tf.float32)</a>
<a class="sourceLine" id="cb3-2" title="2">y <span class="op">=</span> tf.constant(y,dtype<span class="op">=</span>tf.float32)</a></code></pre></div>
<p>We can make the same linear model as above linear model (that gives a probability as an output) by writing a function directly:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1"><span class="co"># Outputs probability of a label being &#39;1&#39;</span></a>
<a class="sourceLine" id="cb4-2" title="2"><span class="kw">def</span> predict(W, b, inputs):</a>
<a class="sourceLine" id="cb4-3" title="3">    <span class="cf">return</span> tf.reshape(</a>
<a class="sourceLine" id="cb4-4" title="4">        tf.nn.sigmoid(</a>
<a class="sourceLine" id="cb4-5" title="5">            tf.add(</a>
<a class="sourceLine" id="cb4-6" title="6">                tf.matmul(inputs, W), b)),(inputs.shape[<span class="dv">0</span>],))</a></code></pre></div>
<p>Here we expect <code>W</code> to be a 2x1 vector of weights, and <code>b</code> to be a scalar bias. You can see that the math - just combining the elements of <code>X</code> according to the weights and adding the bias, is done using TF operations (<code>add</code>, <code>matmul</code>).</p>
<p>We can randomly initialize the model parameters and make a prediction as follows:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">W <span class="op">=</span> tf.reshape(tf.constant(np.random.randn(<span class="dv">2</span>),dtype<span class="op">=</span>tf.float32),(<span class="dv">2</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb5-2" title="2">b <span class="op">=</span> tf.constant(np.random.randn(),dtype<span class="op">=</span>tf.float32)</a>
<a class="sourceLine" id="cb5-3" title="3"></a>
<a class="sourceLine" id="cb5-4" title="4">predict(W,b,X)[<span class="dv">0</span>:<span class="dv">10</span>]</a>
<a class="sourceLine" id="cb5-5" title="5"></a>
<a class="sourceLine" id="cb5-6" title="6">Out[]: <span class="op">&lt;</span>tf.Tensor: shape<span class="op">=</span>(<span class="dv">10</span>,), dtype<span class="op">=</span>float32, numpy<span class="op">=</span></a>
<a class="sourceLine" id="cb5-7" title="7">array([<span class="fl">0.5210591</span> , <span class="fl">0.9974555</span> , <span class="fl">0.63289624</span>, <span class="fl">0.8091879</span> , <span class="fl">0.846755</span>  ,</a>
<a class="sourceLine" id="cb5-8" title="8">       <span class="fl">0.71606183</span>, <span class="fl">0.03832325</span>, <span class="fl">0.95016336</span>, <span class="fl">0.8294958</span> , <span class="fl">0.17246142</span>],</a>
<a class="sourceLine" id="cb5-9" title="9">      dtype<span class="op">=</span>float32)<span class="op">&gt;</span></a></code></pre></div>
<p>Here we explicitly write out the formula for binary cross entropy. To be sure, it is more work and more error prone, and there may be problems running it on a GPU, but we can see exactly what it is doing. We can also create a simple accuracy function:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">inputs <span class="op">=</span> X</a>
<a class="sourceLine" id="cb6-2" title="2">targets <span class="op">=</span> y</a>
<a class="sourceLine" id="cb6-3" title="3"><span class="kw">def</span> loss(W, b):</a>
<a class="sourceLine" id="cb6-4" title="4">    preds <span class="op">=</span> predict(W, b, inputs)</a>
<a class="sourceLine" id="cb6-5" title="5">    label_probs <span class="op">=</span> tf.math.add(</a>
<a class="sourceLine" id="cb6-6" title="6">        tf.math.multiply(preds,targets),</a>
<a class="sourceLine" id="cb6-7" title="7">        tf.math.multiply(<span class="dv">1</span> <span class="op">-</span> preds,<span class="dv">1</span> <span class="op">-</span> targets))</a>
<a class="sourceLine" id="cb6-8" title="8">    <span class="cf">return</span> tf.reduce_mean(<span class="op">-</span>tf.math.log(label_probs<span class="fl">+.0001</span>))</a>
<a class="sourceLine" id="cb6-9" title="9"></a>
<a class="sourceLine" id="cb6-10" title="10"><span class="kw">def</span> accuracy(W,b):</a>
<a class="sourceLine" id="cb6-11" title="11">    preds <span class="op">=</span> np.array(predict(W, b, inputs))</a>
<a class="sourceLine" id="cb6-12" title="12">    <span class="cf">return</span> <span class="bu">sum</span>(np.<span class="bu">round</span>(preds)<span class="op">==</span>np.array(targets))<span class="op">/</span><span class="bu">len</span>(preds)</a></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1">loss(W,b)</a>
<a class="sourceLine" id="cb7-2" title="2"></a>
<a class="sourceLine" id="cb7-3" title="3">Out[]: <span class="op">&lt;</span>tf.Tensor: shape<span class="op">=</span>(), dtype<span class="op">=</span>float32, numpy<span class="op">=</span><span class="fl">0.6823132</span><span class="op">&gt;</span></a>
<a class="sourceLine" id="cb7-4" title="4"></a>
<a class="sourceLine" id="cb7-5" title="5">accuracy(W,b)</a>
<a class="sourceLine" id="cb7-6" title="6"></a>
<a class="sourceLine" id="cb7-7" title="7">Out[]: <span class="fl">0.653</span></a></code></pre></div>
<p>So the only thing we really want to use TF for at this point is to get the gradients. This is done using a <code>tf.GradientTape()</code> to watch the model parameters as we calculate the loss:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="kw">def</span> grad(W,b):</a>
<a class="sourceLine" id="cb8-2" title="2">    <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</a>
<a class="sourceLine" id="cb8-3" title="3">        tape.watch(W)</a>
<a class="sourceLine" id="cb8-4" title="4">        tape.watch(b)</a>
<a class="sourceLine" id="cb8-5" title="5">        loss_value <span class="op">=</span> loss(W,b)</a>
<a class="sourceLine" id="cb8-6" title="6">    </a>
<a class="sourceLine" id="cb8-7" title="7">    <span class="cf">return</span> loss_value, tape.gradient(loss_value, (W,b))</a></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1">grad(W,b)</a>
<a class="sourceLine" id="cb9-2" title="2"></a>
<a class="sourceLine" id="cb9-3" title="3">Out[]: (<span class="op">&lt;</span>tf.Tensor: shape<span class="op">=</span>(), dtype<span class="op">=</span>float32, numpy<span class="op">=</span><span class="fl">0.6823132</span><span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb9-4" title="4"> (<span class="op">&lt;</span>tf.Tensor: shape<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>), dtype<span class="op">=</span>float32, numpy<span class="op">=</span></a>
<a class="sourceLine" id="cb9-5" title="5">  array([[<span class="fl">0.3967318</span> ],</a>
<a class="sourceLine" id="cb9-6" title="6">         [<span class="fl">0.17694217</span>]], dtype<span class="op">=</span>float32)<span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb9-7" title="7">  <span class="op">&lt;</span>tf.Tensor: shape<span class="op">=</span>(), dtype<span class="op">=</span>float32, numpy<span class="op">=</span><span class="fl">0.2413591</span><span class="op">&gt;</span>))</a></code></pre></div>
<p>Above we see the loss repeated again (the first element of the tuple), and gradients for the three parameters.</p>
<p>For training, we do the normal thing of repeatedly calculating gradients, and moving the parameters incrementally against them.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1">rate <span class="op">=</span> <span class="fl">0.01</span></a>
<a class="sourceLine" id="cb10-2" title="2"></a>
<a class="sourceLine" id="cb10-3" title="3">loss_plot <span class="op">=</span> []</a>
<a class="sourceLine" id="cb10-4" title="4"><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</a>
<a class="sourceLine" id="cb10-5" title="5">    loss_value, grads <span class="op">=</span> grad(W,b)</a>
<a class="sourceLine" id="cb10-6" title="6">    W <span class="op">=</span> W <span class="op">-</span> rate <span class="op">*</span> grads[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb10-7" title="7">    b <span class="op">=</span> b <span class="op">-</span> rate <span class="op">*</span> grads[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb10-8" title="8">    </a>
<a class="sourceLine" id="cb10-9" title="9">    loss_plot.append(loss_value)</a>
<a class="sourceLine" id="cb10-10" title="10">    </a>
<a class="sourceLine" id="cb10-11" title="11">plt.plot(loss_plot)</a>
<a class="sourceLine" id="cb10-12" title="12">plt.xlabel(<span class="st">&quot;Iteration&quot;</span>)</a>
<a class="sourceLine" id="cb10-13" title="13">plt.ylabel(<span class="st">&quot;Loss&quot;</span>)</a>
<a class="sourceLine" id="cb10-14" title="14">plt.show()</a>
<a class="sourceLine" id="cb10-15" title="15"><span class="bu">print</span> (<span class="ss">f&quot;loss is </span><span class="sc">{</span>loss_plot[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</a></code></pre></div>
<p><img src="loss_manual_gd.png" title="Loss curve" /></p>
<h2 id="backing-off">Backing off</h2>
<p>There may be reasons you don’t want to write your own layers or optimizer or loss functions. We can trivially add them back in depending on the level we want to work at:</p>
<p><strong>Layers</strong></p>
<p>A Keras layer can be implemented by extending the <code>tf.keras.layers.Layer</code> class. This is covered by a tutorial in the <a href="https://www.tensorflow.org/tutorials/customization/custom_layers">TF documentation</a>, but briefly we need to define <code>__init__</code>, <code>build</code> and <code>call</code> functions that set up the layer and implement it’s internal logic:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1"><span class="kw">class</span> MyDense(tf.keras.layers.Layer):</a>
<a class="sourceLine" id="cb11-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_outputs):</a>
<a class="sourceLine" id="cb11-3" title="3">        <span class="bu">super</span>(MyDense, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb11-4" title="4">        <span class="va">self</span>.num_outputs <span class="op">=</span> num_outputs</a>
<a class="sourceLine" id="cb11-5" title="5">    </a>
<a class="sourceLine" id="cb11-6" title="6">    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</a>
<a class="sourceLine" id="cb11-7" title="7">        <span class="va">self</span>.W <span class="op">=</span> <span class="va">self</span>.add_weight(<span class="st">&quot;W&quot;</span>,</a>
<a class="sourceLine" id="cb11-8" title="8">            shape<span class="op">=</span>[<span class="bu">int</span>(input_shape[<span class="op">-</span><span class="dv">1</span>]), </a>
<a class="sourceLine" id="cb11-9" title="9">            <span class="va">self</span>.num_outputs])</a>
<a class="sourceLine" id="cb11-10" title="10">        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(<span class="st">&quot;b&quot;</span>,shape<span class="op">=</span>[<span class="va">self</span>.num_outputs])</a>
<a class="sourceLine" id="cb11-11" title="11">        </a>
<a class="sourceLine" id="cb11-12" title="12">    <span class="kw">def</span> call(<span class="va">self</span>, <span class="bu">input</span>):</a>
<a class="sourceLine" id="cb11-13" title="13">        <span class="cf">return</span> tf.add(tf.matmul(<span class="bu">input</span>,<span class="va">self</span>.W),<span class="va">self</span>.b)</a></code></pre></div>
<p>The layer needs to be instantiated with an output length specified and called with an appropriately shaped input in order to work:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1">la <span class="op">=</span> MyDense(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb12-2" title="2">la(tf.zeros([<span class="dv">1</span>,<span class="dv">2</span>]))</a>
<a class="sourceLine" id="cb12-3" title="3">la.trainable_variables</a>
<a class="sourceLine" id="cb12-4" title="4"></a>
<a class="sourceLine" id="cb12-5" title="5">Out[]: [<span class="op">&lt;</span>tf.Variable <span class="st">&#39;my_dense_9/W:0&#39;</span> shape<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>) dtype<span class="op">=</span>float32, numpy<span class="op">=</span></a>
<a class="sourceLine" id="cb12-6" title="6"> array([[<span class="op">-</span><span class="fl">0.71072465</span>],</a>
<a class="sourceLine" id="cb12-7" title="7">        [<span class="op">-</span><span class="fl">0.13644254</span>]], dtype<span class="op">=</span>float32)<span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb12-8" title="8"> <span class="op">&lt;</span>tf.Variable <span class="st">&#39;my_dense_9/b:0&#39;</span> shape<span class="op">=</span>(<span class="dv">1</span>,) dtype<span class="op">=</span>float32, numpy<span class="op">=</span>array([<span class="op">-</span><span class="fl">1.4446912</span>], dtype<span class="op">=</span>float32)<span class="op">&gt;</span>]</a>
<a class="sourceLine" id="cb12-9" title="9"></a>
<a class="sourceLine" id="cb12-10" title="10">la(X)</a>
<a class="sourceLine" id="cb12-11" title="11"></a>
<a class="sourceLine" id="cb12-12" title="12">Out[]:</a>
<a class="sourceLine" id="cb12-13" title="13"></a>
<a class="sourceLine" id="cb12-14" title="14"><span class="op">&lt;</span>tf.Tensor: shape<span class="op">=</span>(<span class="dv">1000</span>, <span class="dv">1</span>), dtype<span class="op">=</span>float32, numpy<span class="op">=</span></a>
<a class="sourceLine" id="cb12-15" title="15">array([[<span class="op">-</span><span class="fl">2.67831850e+00</span>],</a>
<a class="sourceLine" id="cb12-16" title="16">       [<span class="op">-</span><span class="fl">1.41732955e+00</span>],</a>
<a class="sourceLine" id="cb12-17" title="17">       [<span class="op">-</span><span class="fl">1.46409166e+00</span>],</a>
<a class="sourceLine" id="cb12-18" title="18">       [<span class="op">-</span><span class="fl">2.22594595e+00</span>],</a>
<a class="sourceLine" id="cb12-19" title="19">...</a></code></pre></div>
<p>Note that we can also access the parameters as <code>la.W</code> and <code>la.b</code>.</p>
<p><strong>Model</strong></p>
<p>To use Keras functionality we wrap the layers in a <code>tf.keras.Model</code>. A basic one that uses the <code>MyDense</code> layer we created is shown below:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" title="1"><span class="kw">class</span> MyModel(tf.keras.Model):</a>
<a class="sourceLine" id="cb13-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb13-3" title="3">        <span class="bu">super</span>(MyModel, <span class="va">self</span>).<span class="fu">__init__</span>(name<span class="op">=</span><span class="st">&#39;&#39;</span>)</a>
<a class="sourceLine" id="cb13-4" title="4">        </a>
<a class="sourceLine" id="cb13-5" title="5">        <span class="va">self</span>.dense <span class="op">=</span> MyDense(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-6" title="6">        _ <span class="op">=</span> <span class="va">self</span>.dense(tf.zeros([<span class="dv">1</span>,<span class="dv">2</span>]))</a>
<a class="sourceLine" id="cb13-7" title="7">        </a>
<a class="sourceLine" id="cb13-8" title="8">    <span class="kw">def</span> call(<span class="va">self</span>, input_tensor, training<span class="op">=</span><span class="va">False</span>):</a>
<a class="sourceLine" id="cb13-9" title="9">        x <span class="op">=</span> <span class="va">self</span>.dense(input_tensor)</a>
<a class="sourceLine" id="cb13-10" title="10">        <span class="cf">return</span> x</a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1">mm <span class="op">=</span> MyModel()</a>
<a class="sourceLine" id="cb14-2" title="2">mm.layers</a>
<a class="sourceLine" id="cb14-3" title="3"></a>
<a class="sourceLine" id="cb14-4" title="4">Out[]: [<span class="op">&lt;</span>__main__.MyDense at <span class="bn">0x7fa823c245e0</span><span class="op">&gt;</span>]</a></code></pre></div>
<p>Here the model does not take any inputs and we have hard coded in the layer shape, but the <code>__init__</code> function can also take inputs. See the TF tutorial linked above for a more comprehensive example of a custom model. The point here is that we have created a minimum model we can use with an optimizer.</p>
<p><strong>Loss</strong></p>
<p>We can use Keras internal loss objects to return a loss. This is equivalent to the earlier loss function that was written out explicitly.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" title="1"><span class="kw">def</span> loss(model, x, y):</a>
<a class="sourceLine" id="cb15-2" title="2">    preds <span class="op">=</span> tf.reshape(tf.nn.sigmoid(model(x)),(x.shape[<span class="dv">0</span>],))</a>
<a class="sourceLine" id="cb15-3" title="3">    targets <span class="op">=</span> y</a>
<a class="sourceLine" id="cb15-4" title="4">    <span class="cf">return</span> tf.keras.losses.BinaryCrossentropy()(y_true<span class="op">=</span>y, y_pred<span class="op">=</span>preds)</a></code></pre></div>
<p><strong>Optimizer</strong></p>
<p>Keras provides optimizers that we can instantiate and use to apply gradients:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1">optimizer <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>)</a></code></pre></div>
<p>We also need to redefine the gradient to account for our newly created model:</p>
<pre><code>def grad(model, inputs, targets):
    with tf.GradientTape() as tape:
        tape.watch(model.trainable_variables)
        loss_value = loss(model, inputs,targets)
    
    return loss_value, tape.gradient(loss_value, model.trainable_variables)</code></pre>
<p>And now we can train the model by calculating a gradient and using the optimizer to apply it to the model parameters.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" title="1"><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</a>
<a class="sourceLine" id="cb18-2" title="2">    loss_value, grads <span class="op">=</span> grad(mm, X, y)</a>
<a class="sourceLine" id="cb18-3" title="3">    optimizer.apply_gradients(<span class="bu">zip</span>(grads, mm.trainable_variables))</a>
<a class="sourceLine" id="cb18-4" title="4">    </a>
<a class="sourceLine" id="cb18-5" title="5">    loss_plot.append(loss_value)</a></code></pre></div>
<p><img src="loss_mydense.png" title="Loss curve" /></p>
<p>An additional thing that the Keras boilerplate takes care of it the batching of inputs, not covered here, but easy to add if you understand the manual training process.</p>
<p>In conclusion, we started with some pretty standard TF/Keras functions to train a model:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" title="1">model <span class="op">=</span> tf.keras.models.Sequential([tf.keras.layers.Dense(<span class="dv">1</span>,input_shape<span class="op">=</span>(<span class="dv">2</span>,))])</a>
<a class="sourceLine" id="cb19-2" title="2">model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;SGD&#39;</span>,</a>
<a class="sourceLine" id="cb19-3" title="3">    loss<span class="op">=</span>tf.keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</a>
<a class="sourceLine" id="cb19-4" title="4">    metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</a>
<a class="sourceLine" id="cb19-5" title="5">model.fit(X,y,batch_size<span class="op">=</span><span class="dv">100</span>,epochs<span class="op">=</span><span class="dv">20</span>)</a></code></pre></div>
<p>We showed how to build up to this level of abstraction from “scratch”, using only automatic differentiation. For some experience levels and backgrounds, I hope that understanding this buildup is helpful as a first step to learning how to use TF, and can provide additional insight compared to just learning the API.</p>
<p>Please let me know if you have any questions or comments!</p>
